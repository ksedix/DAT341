{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-grill",
   "metadata": {},
   "source": [
    "# Assignment 4 DAT341 - Implementing Linear Classifiers\n",
    "## Assignment page: https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/assignment4.html#foot1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-volleyball",
   "metadata": {},
   "source": [
    "## Group Members: Mirco Ghadri, Tobias Filmberg, Sameer Jathavedan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-timothy",
   "metadata": {},
   "source": [
    "### Date: 24-2-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-graham",
   "metadata": {},
   "source": [
    "# Exercise Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structural-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clinical-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "empty-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for X1:  1.0\n",
      "Training accuracy for X2:  0.5\n"
     ]
    }
   ],
   "source": [
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(\"Training accuracy for X1: \",accuracy_score(Y1, guesses1))\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(\"Training accuracy for X2: \",accuracy_score(Y2, guesses2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-supply",
   "metadata": {},
   "source": [
    "### Q: Why does the classifier get a 100% training accuracy score for X1 but only a 50% accuracy score for predicting X2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-narrow",
   "metadata": {},
   "source": [
    "### The reason the perceptron gets a 100% training accuracy on the first dataset(X1) is because the data is linearly separable. The reason the perceptron only gets a 50% accuracy on the second dataset(X2) is because the data is not linearly separable. It can be shown with a plot that the data in the first dataset is linearly separable and that the data in the second dataset is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-poetry",
   "metadata": {},
   "source": [
    "## Dataset n.1: X1 and Y1 - Linearly Separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-climate",
   "metadata": {},
   "source": [
    "The features(X) of the dataset(the month and the city) are in this case plotted on the x and y axis. The output label(Y) is shown with the letter R, which stands for Rainy weather, and the letter S, which stands for Sunny weather. We can see that the data is linearly separable since we can draw a straight line that separates it into 2 homogenous subsets - rainy weather and sunny weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-framework",
   "metadata": {},
   "source": [
    "<img src=\"linear_separability.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-chain",
   "metadata": {},
   "source": [
    "## Dataset n.2: X2 and Y2 - Not Linearly Separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-nightlife",
   "metadata": {},
   "source": [
    "In the case of X2 and Y2, it is not possible for the Perceptron to create a model(line) that separates the output labels(Y) into 2 distinct and homogenous subsets. The best possible accuracy it can get is 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-employee",
   "metadata": {},
   "source": [
    "<img src=\"linear_inseparability.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recreational-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-gambling",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bright-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-irish",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lucky-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                # why does it do <=0. Can the score be equal to 0? The score can only be -1 or 1.\n",
    "                if y*score <= 0:\n",
    "                    #subtract the feature vector from the weight vector if the predicted y value was positive but the actual was negative. Add the feature vector to the weight vector if the predicted y value was negative but the real was positive.\n",
    "                    self.w += y*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-photography",
   "metadata": {},
   "source": [
    "### Perceptron Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparsePerceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "                        \n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-european",
   "metadata": {},
   "source": [
    "## Document Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mounted-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.22 sec.\n",
      "Accuracy: 0.8300.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "    SparsePerceptron()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-venice",
   "metadata": {},
   "source": [
    "## Implementing the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "combined-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the svc(support vector classifier) learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the svc learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        #the value of the regularizer is 1 divided by the number of instances in the training set, len(Y)\n",
    "        regularizer = 1/len(Y)\n",
    "        t = 1\n",
    "        \n",
    "        # SVC algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                \n",
    "                # Compute eta. t starts from 1 because we can not divide by 0\n",
    "                # the value of eta is large at first, but gradually gets smaller. \n",
    "                eta = 1 / (regularizer * t)\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                #the bug was caused because we incremented the variable t for each iteration of the outer for loop.\n",
    "                #the variable t should be incremented for each iteration of the inner for loop.\n",
    "                t+=1\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    self.w = (1-eta * regularizer)*self.w + (eta * y) * x \n",
    "                else:\n",
    "                    self.w = (1-eta * regularizer)*self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "jewish-cholesterol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.08 sec.\n",
      "Accuracy: 0.8317.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # SVC classifier that we created\n",
    "    SVC()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-illinois",
   "metadata": {},
   "source": [
    "### We land at an accuracy of 0.8317 when using Pegasos algorithm to create the SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-partition",
   "metadata": {},
   "source": [
    "### Bonus task 1a) Faster linear algebra operations using BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fleet-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "given-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC_BLAS(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the svc(support vector classifier) learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the svc learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        #the value of the regularizer is 1 divided by the number of instances in the training set, len(Y)\n",
    "        regularizer = 1/len(Y)\n",
    "        t=1\n",
    "        \n",
    "        # SVC algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                eta = 1 / (regularizer * t)\n",
    "                # Compute the output score for this instance.\n",
    "                score = ddot(x,self.w)\n",
    "                \n",
    "                #the bug in the previous code was caused by the fact that t was incremented in the outer for loop\n",
    "                #t should be incremented in the inner for loop\n",
    "                t+=1\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    dscal(1-eta * regularizer, self.w)\n",
    "                    daxpy(x,self.w,a=eta*y)\n",
    "                else:\n",
    "                    dscal(1-eta * regularizer, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "funky-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.22 sec.\n",
      "Accuracy: 0.8326.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # SVC classifier that we created\n",
    "    SVC_BLAS()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-girlfriend",
   "metadata": {},
   "source": [
    "The accuracy when running the BLAS functions inside of the SVC is the same: 0.8326.  \n",
    "However, the computation time has improved to 1.22 seconds compared to 2 seconds previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-arlington",
   "metadata": {},
   "source": [
    "### Bonus task 1b) SparseSVC - an implementation of SVC that deals with sparse training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aggressive-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the svc(support vector classifier) learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the svc learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        #the value of the regularizer is 1 divided by the number of instances in the training set, len(Y)\n",
    "        regularizer = 1/len(Y)\n",
    "        t=1\n",
    "        \n",
    "        # SVC algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                eta = 1 / (regularizer * t)\n",
    "                \n",
    "                # Compute the output score for this instance.\n",
    "                score = sparse_dense_dot(x,self.w)\n",
    "                t+=1\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    dscal(1-eta * regularizer, self.w)\n",
    "                    add_sparse_to_dense(x,self.w,eta*y)\n",
    "                else:\n",
    "                    #vector rescaling\n",
    "                    dscal(1-eta * regularizer, self.w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "coastal-composition",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.04 sec.\n",
      "Accuracy: 0.8414.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    Normalizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    SparseSVC()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-gossip",
   "metadata": {},
   "source": [
    "When running the document classifier without SelectKBest(k=1000) and using a simple SVC, the running time is 23.77 seconds. However the accuracy has increased to 0.8410.  \n",
    "When running the document classifier without SelectKBest(k=1000) but using SparseSVC, we see a significant speed improvement. The training time is now only **6 seconds and the accuracy is also 0.8414**.\n",
    "\n",
    "The reason for this significant improvement in computation speed when using sparseSVC is that it does not consider the empty parts of the sparse vector(in this case the training data) when it performs addition and multiplication. This allows it to compute its results much faster. For example, when it takes the dot product between the sparse vector x and the dense weight vector, it will not consider the parts in the sparse vector x that are 0 when it computes the dot product.\n",
    "\n",
    "In order for our sparseSVC implementation to work correctly, we also had to remove the line of code in the SVC that was: \n",
    "```if not isinstance(X, np.ndarray):\n",
    "    X = X.toarray()```    \n",
    "\n",
    "The reason is that we want the training data to be in a sparse matrix format and not a regular numpy array. So we do not want to convert it to a regular numpy matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-significance",
   "metadata": {},
   "source": [
    "### Bonus task 1c) SparseSVCOptimized - an optimized implementation of SparseSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-practitioner",
   "metadata": {},
   "source": [
    "There is yet another way to speed up the training time. This method is described in section 4 of the clarification document. Instead of scaling the weight vector directly, we accumulate the scaling factors and scale the vector at the end. This is because scaling the vector for each iteration is cost-heavy, especially if it is a large vector. So we only scale the vector 1 time at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "proprietary-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSVCOptimized(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the svc(support vector classifier) learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the svc learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        #the value of the regularizer is 1 divided by the number of instances in the training set, len(Y)\n",
    "        regularizer = 1/len(Y)\n",
    "        a=1\n",
    "        #we set to 2, because if we set it to 1, a will become 0 and division by 0 is not allowed\n",
    "        t=2\n",
    "        # SVC algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                eta = 1 / (regularizer * t)\n",
    "                #instead of scaling self.w and then multiplying it by x, simply multiply the dot product by the scaling factor\n",
    "                score = a*sparse_dense_dot(x,self.w)\n",
    "                a = (1-eta*regularizer)*a     \n",
    "                t+=1\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    add_sparse_to_dense(x,self.w,eta*y/a)\n",
    "        self.w = self.w*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "limiting-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.08 sec.\n",
      "Accuracy: 0.8397.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    Normalizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    SparseSVCOptimized()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-lambda",
   "metadata": {},
   "source": [
    "After optimizing the SparseSVC implementation, we gain 1 second of faster training without SelectKBest(k=1000).\n",
    "Final result:\n",
    "Training time: **5 sec**.\n",
    "Accuracy: **0.8397**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-judge",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "crude-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the LR(logistic regression) learning algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the logistic regression learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        #the value of the regularizer is 1 divided by the number of instances in the training set, len(Y)\n",
    "        regularizer = 1/len(Y)\n",
    "        t=1\n",
    "        \n",
    "        # SVC algorithm:\n",
    "        for t in range(1,self.n_iter+1):\n",
    "            for x, y in zip(X, Ye):\n",
    "                eta = 1 / (regularizer * t)\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                t+=1\n",
    "                log_loss_gradient = (-y/(1+np.exp(y*score)))*x\n",
    "                gradient = regularizer * self.w + log_loss_gradient\n",
    "                # If there was an error, update the weights.\n",
    "                self.w = self.w - eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sitting-holmes",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.01 sec.\n",
      "Accuracy: 0.8229.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # SVC classifier that we created\n",
    "    LR()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
